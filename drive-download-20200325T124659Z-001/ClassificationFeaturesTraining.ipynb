{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"ClassificationFeaturesTraining.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"k9FApNOD10Gf","colab_type":"text"},"source":["# Data download"]},{"cell_type":"markdown","metadata":{"id":"puDtABkc10Gl","colab_type":"text"},"source":["1. Open terminal\n","2. Go to cs231n/datasets\n","3. Run get_datasets.sh\n"]},{"cell_type":"code","metadata":{"id":"BPIEilEb117C","colab_type":"code","outputId":"f4ba1489-1687-47fd-b442-e874061a3a20","executionInfo":{"status":"ok","timestamp":1582653655946,"user_tz":-240,"elapsed":648,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cGtRRBDC2AYq","colab_type":"code","outputId":"8b696769-2459-40a2-b7d2-2585f5fdaf4e","executionInfo":{"status":"ok","timestamp":1582653667808,"user_tz":-240,"elapsed":12254,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["%%shell\n","bash -x /content/gdrive/My\\ Drive/homework1/cs231n/datasets/get_datasets.sh"],"execution_count":2,"outputs":[{"output_type":"stream","text":["+ wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","--2020-02-25 18:00:57--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170498071 (163M) [application/x-gzip]\n","Saving to: ‘cifar-10-python.tar.gz’\n","\n","cifar-10-python.tar 100%[===================>] 162.60M  24.4MB/s    in 7.2s    \n","\n","2020-02-25 18:01:05 (22.6 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n","\n","+ tar -xzvf cifar-10-python.tar.gz\n","cifar-10-batches-py/\n","cifar-10-batches-py/data_batch_4\n","cifar-10-batches-py/readme.html\n","cifar-10-batches-py/test_batch\n","cifar-10-batches-py/data_batch_3\n","cifar-10-batches-py/batches.meta\n","cifar-10-batches-py/data_batch_2\n","cifar-10-batches-py/data_batch_5\n","cifar-10-batches-py/data_batch_1\n","+ rm cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"M7NAGDPW4Vj7","colab_type":"text"},"source":["# Helper python modules\n"]},{"cell_type":"code","metadata":{"id":"HVbZAz-_4-Xr","colab_type":"code","outputId":"637034ea-9326-4d47-c66a-fcee0fc38ce3","executionInfo":{"status":"ok","timestamp":1582653675416,"user_tz":-240,"elapsed":19495,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["!pip install Pillow\n","!pip install scipy==1.1.0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (6.2.2)\n","Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.17.5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xM7yRf4V5iEm","colab_type":"text"},"source":["data_utils\n"]},{"cell_type":"code","metadata":{"id":"5HdFPQN-4d4b","colab_type":"code","cellView":"form","colab":{}},"source":["#@title data_utils\n","\n","from __future__ import print_function\n","\n","from six.moves import cPickle as pickle\n","import numpy as np\n","import os\n","from scipy.misc import imread\n","import platform\n","\n","\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","\n","def load_CIFAR10(ROOT):\n","  \"\"\" load all of cifar \"\"\"\n","  xs = []\n","  ys = []\n","  for b in range(1,6):\n","    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n","    X, Y = load_CIFAR_batch(f)\n","    xs.append(X)\n","    ys.append(Y)    \n","  Xtr = np.concatenate(xs)\n","  Ytr = np.concatenate(ys)\n","  del X, Y\n","  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n","  return Xtr, Ytr, Xte, Yte\n","\n","\n","def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,\n","                     subtract_mean=True):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for classifiers. These are the same steps as we used for the SVM, but\n","    condensed to a single function.\n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","        \n","    # Subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","\n","    # Normalize the data: subtract the mean image\n","    if subtract_mean:\n","      mean_image = np.mean(X_train, axis=0)\n","      X_train -= mean_image\n","      X_val -= mean_image\n","      X_test -= mean_image\n","    \n","    # Transpose so that channels come first\n","    X_train = X_train.transpose(0, 3, 1, 2).copy()\n","    X_val = X_val.transpose(0, 3, 1, 2).copy()\n","    X_test = X_test.transpose(0, 3, 1, 2).copy()\n","\n","    # Package data into a dictionary\n","    return {\n","      'X_train': X_train, 'y_train': y_train,\n","      'X_val': X_val, 'y_val': y_val,\n","      'X_test': X_test, 'y_test': y_test,\n","    }\n","    \n","\n","def load_tiny_imagenet(path, dtype=np.float32, subtract_mean=True):\n","  \"\"\"\n","  Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n","  TinyImageNet-200 have the same directory structure, so this can be used\n","  to load any of them.\n","\n","  Inputs:\n","  - path: String giving path to the directory to load.\n","  - dtype: numpy datatype used to load the data.\n","  - subtract_mean: Whether to subtract the mean training image.\n","\n","  Returns: A dictionary with the following entries:\n","  - class_names: A list where class_names[i] is a list of strings giving the\n","    WordNet names for class i in the loaded dataset.\n","  - X_train: (N_tr, 3, 64, 64) array of training images\n","  - y_train: (N_tr,) array of training labels\n","  - X_val: (N_val, 3, 64, 64) array of validation images\n","  - y_val: (N_val,) array of validation labels\n","  - X_test: (N_test, 3, 64, 64) array of testing images.\n","  - y_test: (N_test,) array of test labels; if test labels are not available\n","    (such as in student code) then y_test will be None.\n","  - mean_image: (3, 64, 64) array giving mean training image\n","  \"\"\"\n","  # First load wnids\n","  with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n","    wnids = [x.strip() for x in f]\n","\n","  # Map wnids to integer labels\n","  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n","\n","  # Use words.txt to get names for each class\n","  with open(os.path.join(path, 'words.txt'), 'r') as f:\n","    wnid_to_words = dict(line.split('\\t') for line in f)\n","    for wnid, words in wnid_to_words.iteritems():\n","      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n","  class_names = [wnid_to_words[wnid] for wnid in wnids]\n","\n","  # Next load training data.\n","  X_train = []\n","  y_train = []\n","  for i, wnid in enumerate(wnids):\n","    if (i + 1) % 20 == 0:\n","      print('loading training data for synset %d / %d' % (i + 1, len(wnids)))\n","    # To figure out the filenames we need to open the boxes file\n","    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n","    with open(boxes_file, 'r') as f:\n","      filenames = [x.split('\\t')[0] for x in f]\n","    num_images = len(filenames)\n","    \n","    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n","    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n","    for j, img_file in enumerate(filenames):\n","      img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n","      img = imread(img_file)\n","      if img.ndim == 2:\n","        ## grayscale file\n","        img.shape = (64, 64, 1)\n","      X_train_block[j] = img.transpose(2, 0, 1)\n","    X_train.append(X_train_block)\n","    y_train.append(y_train_block)\n","      \n","  # We need to concatenate all training data\n","  X_train = np.concatenate(X_train, axis=0)\n","  y_train = np.concatenate(y_train, axis=0)\n","  \n","  # Next load validation data\n","  with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n","    img_files = []\n","    val_wnids = []\n","    for line in f:\n","      img_file, wnid = line.split('\\t')[:2]\n","      img_files.append(img_file)\n","      val_wnids.append(wnid)\n","    num_val = len(img_files)\n","    y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n","    X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n","    for i, img_file in enumerate(img_files):\n","      img_file = os.path.join(path, 'val', 'images', img_file)\n","      img = imread(img_file)\n","      if img.ndim == 2:\n","        img.shape = (64, 64, 1)\n","      X_val[i] = img.transpose(2, 0, 1)\n","\n","  # Next load test images\n","  # Students won't have test labels, so we need to iterate over files in the\n","  # images directory.\n","  img_files = os.listdir(os.path.join(path, 'test', 'images'))\n","  X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n","  for i, img_file in enumerate(img_files):\n","    img_file = os.path.join(path, 'test', 'images', img_file)\n","    img = imread(img_file)\n","    if img.ndim == 2:\n","      img.shape = (64, 64, 1)\n","    X_test[i] = img.transpose(2, 0, 1)\n","\n","  y_test = None\n","  y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n","  if os.path.isfile(y_test_file):\n","    with open(y_test_file, 'r') as f:\n","      img_file_to_wnid = {}\n","      for line in f:\n","        line = line.split('\\t')\n","        img_file_to_wnid[line[0]] = line[1]\n","    y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]\n","    y_test = np.array(y_test)\n","  \n","  mean_image = X_train.mean(axis=0)\n","  if subtract_mean:\n","    X_train -= mean_image[None]\n","    X_val -= mean_image[None]\n","    X_test -= mean_image[None]\n","\n","  return {\n","    'class_names': class_names,\n","    'X_train': X_train,\n","    'y_train': y_train,\n","    'X_val': X_val,\n","    'y_val': y_val,\n","    'X_test': X_test,\n","    'y_test': y_test,\n","    'class_names': class_names,\n","    'mean_image': mean_image,\n","  }\n","\n","\n","def load_models(models_dir):\n","  \"\"\"\n","  Load saved models from disk. This will attempt to unpickle all files in a\n","  directory; any files that give errors on unpickling (such as README.txt) will\n","  be skipped.\n","\n","  Inputs:\n","  - models_dir: String giving the path to a directory containing model files.\n","    Each model file is a pickled dictionary with a 'model' field.\n","\n","  Returns:\n","  A dictionary mapping model file names to models.\n","  \"\"\"\n","  models = {}\n","  for model_file in os.listdir(models_dir):\n","    with open(os.path.join(models_dir, model_file), 'rb') as f:\n","      try:\n","        models[model_file] = load_pickle(f)['model']\n","      except pickle.UnpicklingError:\n","        continue\n","  return models\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCDekj_O7sc2","colab_type":"text"},"source":["features"]},{"cell_type":"code","metadata":{"id":"eo_IhFms7vph","colab_type":"code","cellView":"form","colab":{}},"source":["#@title features\n","from __future__ import print_function\n","\n","import matplotlib\n","import numpy as np\n","from scipy.ndimage import uniform_filter\n","\n","\n","def extract_features(imgs, feature_fns, verbose=False):\n","  \"\"\"\n","  Given pixel data for images and several feature functions that can operate on\n","  single images, apply all feature functions to all images, concatenating the\n","  feature vectors for each image and storing the features for all images in\n","  a single matrix.\n","\n","  Inputs:\n","  - imgs: N x H X W X C array of pixel data for N images.\n","  - feature_fns: List of k feature functions. The ith feature function should\n","    take as input an H x W x D array and return a (one-dimensional) array of\n","    length F_i.\n","  - verbose: Boolean; if true, print progress.\n","\n","  Returns:\n","  An array of shape (N, F_1 + ... + F_k) where each column is the concatenation\n","  of all features for a single image.\n","  \"\"\"\n","  num_images = imgs.shape[0]\n","  if num_images == 0:\n","    return np.array([])\n","\n","  # Use the first image to determine feature dimensions\n","  feature_dims = []\n","  first_image_features = []\n","  for feature_fn in feature_fns:\n","    feats = feature_fn(imgs[0].squeeze())\n","    assert len(feats.shape) == 1, 'Feature functions must be one-dimensional'\n","    feature_dims.append(feats.size)\n","    first_image_features.append(feats)\n","\n","  # Now that we know the dimensions of the features, we can allocate a single\n","  # big array to store all features as columns.\n","  total_feature_dim = sum(feature_dims)\n","  imgs_features = np.zeros((num_images, total_feature_dim))\n","  imgs_features[0] = np.hstack(first_image_features).T\n","\n","  # Extract features for the rest of the images.\n","  for i in range(1, num_images):\n","    idx = 0\n","    for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n","      next_idx = idx + feature_dim\n","      imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n","      idx = next_idx\n","    if verbose and i % 1000 == 0:\n","      print('Done extracting features for %d / %d images' % (i, num_images))\n","\n","  return imgs_features\n","\n","\n","def rgb2gray(rgb):\n","  \"\"\"Convert RGB image to grayscale\n","\n","    Parameters:\n","      rgb : RGB image\n","\n","    Returns:\n","      gray : grayscale image\n","  \n","  \"\"\"\n","  return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n","\n","\n","def hog_feature(im):\n","  \"\"\"Compute Histogram of Gradient (HOG) feature for an image\n","  \n","       Modified from skimage.feature.hog\n","       http://pydoc.net/Python/scikits-image/0.4.2/skimage.feature.hog\n","     \n","     Reference:\n","       Histograms of Oriented Gradients for Human Detection\n","       Navneet Dalal and Bill Triggs, CVPR 2005\n","     \n","    Parameters:\n","      im : an input grayscale or rgb image\n","      \n","    Returns:\n","      feat: Histogram of Gradient (HOG) feature\n","    \n","  \"\"\"\n","  \n","  # convert rgb to grayscale if needed\n","  if im.ndim == 3:\n","    image = rgb2gray(im)\n","  else:\n","    image = np.at_least_2d(im)\n","\n","  sx, sy = image.shape # image size\n","  orientations = 9 # number of gradient bins\n","  cx, cy = (8, 8) # pixels per cell\n","\n","  gx = np.zeros(image.shape)\n","  gy = np.zeros(image.shape)\n","  gx[:, :-1] = np.diff(image, n=1, axis=1) # compute gradient on x-direction\n","  gy[:-1, :] = np.diff(image, n=1, axis=0) # compute gradient on y-direction\n","  grad_mag = np.sqrt(gx ** 2 + gy ** 2) # gradient magnitude\n","  grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90 # gradient orientation\n","\n","  n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n","  n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n","  # compute orientations integral images\n","  orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n","  for i in range(orientations):\n","    # create new integral image for this orientation\n","    # isolate orientations in this range\n","    temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n","                        grad_ori, 0)\n","    temp_ori = np.where(grad_ori >= 180 / orientations * i,\n","                        temp_ori, 0)\n","    # select magnitudes for those orientations\n","    cond2 = temp_ori > 0\n","    temp_mag = np.where(cond2, grad_mag, 0)\n","    orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx/2)::cx, int(cy/2)::cy].T\n","  \n","  return orientation_histogram.ravel()\n","\n","\n","def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n","  \"\"\"\n","  Compute color histogram for an image using hue.\n","\n","  Inputs:\n","  - im: H x W x C array of pixel data for an RGB image.\n","  - nbin: Number of histogram bins. (default: 10)\n","  - xmin: Minimum pixel value (default: 0)\n","  - xmax: Maximum pixel value (default: 255)\n","  - normalized: Whether to normalize the histogram (default: True)\n","\n","  Returns:\n","    1D vector of length nbin giving the color histogram over the hue of the\n","    input image.\n","  \"\"\"\n","  ndim = im.ndim\n","  bins = np.linspace(xmin, xmax, nbin+1)\n","  hsv = matplotlib.colors.rgb_to_hsv(im/xmax) * xmax\n","  imhist, bin_edges = np.histogram(hsv[:,:,0], bins=bins, density=normalized)\n","  imhist = imhist * np.diff(bin_edges)\n","\n","  # return histogram\n","  return imhist\n","\n","\n","pass\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-YUJnSu7_HQ","colab_type":"text"},"source":["gradient_check"]},{"cell_type":"code","metadata":{"id":"2XnULeTo8BrP","colab_type":"code","cellView":"form","colab":{}},"source":["#@title gradient_check\n","from __future__ import print_function\n","\n","import numpy as np\n","from random import randrange\n","\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","  \"\"\" \n","  a naive implementation of numerical gradient of f at x \n","  - f should be a function that takes a single argument\n","  - x is the point (numpy array) to evaluate the gradient at\n","  \"\"\" \n","\n","  fx = f(x) # evaluate function value at original point\n","  grad = np.zeros_like(x)\n","  # iterate over all indexes in x\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","\n","    # evaluate function at x+h\n","    ix = it.multi_index\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evalute f(x + h)\n","    x[ix] = oldval - h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # restore\n","\n","    # compute the partial derivative with centered formula\n","    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","    if verbose:\n","      print(ix, grad[ix])\n","    it.iternext() # step to next dimension\n","\n","  return grad\n","\n","\n","def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","  \"\"\"\n","  Evaluate a numeric gradient for a function that accepts a numpy\n","  array and returns a numpy array.\n","  \"\"\"\n","  grad = np.zeros_like(x)\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","    ix = it.multi_index\n","    \n","    oldval = x[ix]\n","    x[ix] = oldval + h\n","    pos = f(x).copy()\n","    x[ix] = oldval - h\n","    neg = f(x).copy()\n","    x[ix] = oldval\n","    \n","    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","    it.iternext()\n","  return grad\n","\n","\n","def eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n","  \"\"\"\n","  Compute numeric gradients for a function that operates on input\n","  and output blobs.\n","  \n","  We assume that f accepts several input blobs as arguments, followed by a blob\n","  into which outputs will be written. For example, f might be called like this:\n","\n","  f(x, w, out)\n","  \n","  where x and w are input Blobs, and the result of f will be written to out.\n","\n","  Inputs: \n","  - f: function\n","  - inputs: tuple of input blobs\n","  - output: output blob\n","  - h: step size\n","  \"\"\"\n","  numeric_diffs = []\n","  for input_blob in inputs:\n","    diff = np.zeros_like(input_blob.diffs)\n","    it = np.nditer(input_blob.vals, flags=['multi_index'],\n","                   op_flags=['readwrite'])\n","    while not it.finished:\n","      idx = it.multi_index\n","      orig = input_blob.vals[idx]\n","\n","      input_blob.vals[idx] = orig + h\n","      f(*(inputs + (output,)))\n","      pos = np.copy(output.vals)\n","      input_blob.vals[idx] = orig - h\n","      f(*(inputs + (output,)))\n","      neg = np.copy(output.vals)\n","      input_blob.vals[idx] = orig\n","      \n","      diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n","\n","      it.iternext()\n","    numeric_diffs.append(diff)\n","  return numeric_diffs\n","\n","\n","def eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n","  return eval_numerical_gradient_blobs(lambda *args: net.forward(),\n","              inputs, output, h=h)\n","\n","\n","def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n","  \"\"\"\n","  sample a few random elements and only return numerical\n","  in this dimensions.\n","  \"\"\"\n","\n","  for i in range(num_checks):\n","    ix = tuple([randrange(m) for m in x.shape])\n","\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evaluate f(x + h)\n","    x[ix] = oldval - h # increment by h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # reset\n","\n","    grad_numerical = (fxph - fxmh) / (2 * h)\n","    grad_analytic = analytic_grad[ix]\n","    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n","    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q56OsZQf8HZ3","colab_type":"text"},"source":["vis_utils"]},{"cell_type":"code","metadata":{"id":"VnM45bmj8I9h","colab_type":"code","cellView":"form","colab":{}},"source":["#@title vis_utils\n","from math import sqrt, ceil\n","import numpy as np\n","\n","def visualize_grid(Xs, ubound=255.0, padding=1):\n","  \"\"\"\n","  Reshape a 4D tensor of image data to a grid for easy visualization.\n","\n","  Inputs:\n","  - Xs: Data of shape (N, H, W, C)\n","  - ubound: Output grid will have values scaled to the range [0, ubound]\n","  - padding: The number of blank pixels between elements of the grid\n","  \"\"\"\n","  (N, H, W, C) = Xs.shape\n","  grid_size = int(ceil(sqrt(N)))\n","  grid_height = H * grid_size + padding * (grid_size - 1)\n","  grid_width = W * grid_size + padding * (grid_size - 1)\n","  grid = np.zeros((grid_height, grid_width, C))\n","  next_idx = 0\n","  y0, y1 = 0, H\n","  for y in range(grid_size):\n","    x0, x1 = 0, W\n","    for x in range(grid_size):\n","      if next_idx < N:\n","        img = Xs[next_idx]\n","        low, high = np.min(img), np.max(img)\n","        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n","        # grid[y0:y1, x0:x1] = Xs[next_idx]\n","        next_idx += 1\n","      x0 += W + padding\n","      x1 += W + padding\n","    y0 += H + padding\n","    y1 += H + padding\n","  # grid_max = np.max(grid)\n","  # grid_min = np.min(grid)\n","  # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n","  return grid\n","\n","def vis_grid(Xs):\n","  \"\"\" visualize a grid of images \"\"\"\n","  (N, H, W, C) = Xs.shape\n","  A = int(ceil(sqrt(N)))\n","  G = np.ones((A*H+A, A*W+A, C), Xs.dtype)\n","  G *= np.min(Xs)\n","  n = 0\n","  for y in range(A):\n","    for x in range(A):\n","      if n < N:\n","        G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = Xs[n,:,:,:]\n","        n += 1\n","  # normalize to [0,1]\n","  maxg = G.max()\n","  ming = G.min()\n","  G = (G - ming)/(maxg-ming)\n","  return G\n","  \n","def vis_nn(rows):\n","  \"\"\" visualize array of arrays of images \"\"\"\n","  N = len(rows)\n","  D = len(rows[0])\n","  H,W,C = rows[0][0].shape\n","  Xs = rows[0][0]\n","  G = np.ones((N*H+N, D*W+D, C), Xs.dtype)\n","  for y in range(N):\n","    for x in range(D):\n","      G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = rows[y][x]\n","  # normalize to [0,1]\n","  maxg = G.max()\n","  ming = G.min()\n","  G = (G - ming)/(maxg-ming)\n","  return G\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNhLN_MLQBQ3","colab_type":"text"},"source":["K nearest neighbor"]},{"cell_type":"code","metadata":{"id":"-lLtQhFlQFiD","colab_type":"code","cellView":"form","colab":{}},"source":["#@title K nearest neighbor\n","import numpy as np\n","\n","class KNearestNeighbor(object):\n","\t\"\"\" a kNN classifier with L2 distance \"\"\"\n","\n","\tdef __init__(self):\n","\t\tpass\n","\n","\tdef train(self, X, y):\n","\t\t\"\"\"\n","\t\tTrain the classifier. For k-nearest neighbors this is just \n","\t\tmemorizing the training data.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (num_train, D) containing the training data\n","\t\t\tconsisting of num_train samples each of dimension D.\n","\t\t- y: A numpy array of shape (N,) containing the training labels, where\n","\t\t\t\t y[i] is the label for X[i].\n","\t\t\"\"\"\n","\t\tself.X_train = X\n","\t\tself.y_train = y\n","\t\t\n","\tdef predict(self, X, k=1, num_loops=0):\n","\t\t\"\"\"\n","\t\tPredict labels for test data using this classifier.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (num_test, D) containing test data consisting\n","\t\t\t\t of num_test samples each of dimension D.\n","\t\t- k: The number of nearest neighbors that vote for the predicted labels.\n","\t\t- num_loops: Determines which implementation to use to compute distances\n","\t\t\tbetween training points and testing points.\n","\n","\t\tReturns:\n","\t\t- y: A numpy array of shape (num_test,) containing predicted labels for the\n","\t\t\ttest data, where y[i] is the predicted label for the test point X[i].  \n","\t\t\"\"\"\n","\t\tif num_loops == 0:\n","\t\t\tdists = self.compute_distances_no_loops(X)\n","\t\telif num_loops == 1:\n","\t\t\tdists = self.compute_distances_one_loop(X)\n","\t\telif num_loops == 2:\n","\t\t\tdists = self.compute_distances_two_loops(X)\n","\t\telse:\n","\t\t\traise ValueError('Invalid value %d for num_loops' % num_loops)\n","\n","\t\treturn self.predict_labels(dists, k=k)\n","\n","\tdef compute_distances_two_loops(self, X):\n","\t\t\"\"\"\n","\t\tCompute the distance between each test point in X and each training point\n","\t\tin self.X_train using a nested loop over both the training data and the \n","\t\ttest data.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (num_test, D) containing test data.\n","\n","\t\tReturns:\n","\t\t- dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n","\t\t\tis the Euclidean distance between the ith test point and the jth training\n","\t\t\tpoint.\n","\t\t\"\"\"\n","\t\tnum_test = X.shape[0]\n","\t\tnum_train = self.X_train.shape[0]\n","\t\tdists = np.zeros((num_test, num_train))\n","\t\tfor i in range(num_test):\n","\t\t\tfor j in range(num_train):\n","\t\t\t\t#####################################################################\n","\t\t\t\t# TODO:                                                             #\n","\t\t\t\t# Compute the l2 distance between the ith test point and the jth    #\n","\t\t\t\t# training point, and store the result in dists[i, j]. You should   #\n","\t\t\t\t# not use a loop over dimension.                                    #\n","\t\t\t\t#####################################################################\n","\t\t\t\ta = self.X_train[i]\n","\t\t\t\tb = X[j]\n","\t\t\t\tdistance = np.linalg.norm(a-b)\n","\t\t\t\tdists[i, j] = distance\n","\t\t\t\t#####################################################################\n","\t\t\t\t#                       END OF YOUR CODE                            #\n","\t\t\t\t#####################################################################\n","\t\treturn dists\n","\n","\tdef compute_distances_one_loop(self, X):\n","\t\t\"\"\"\n","\t\tCompute the distance between each test point in X and each training point\n","\t\tin self.X_train using a single loop over the test data.\n","\n","\t\tInput / Output: Same as compute_distances_two_loops\n","\t\t\"\"\"\n","\t\tnum_test = X.shape[0]\n","\t\tnum_train = self.X_train.shape[0]\n","\t\tdists = np.zeros((num_test, num_train))\n","\t\tfor i in range(num_test):\n","\t\t\t#######################################################################\n","\t\t\t# TODO:                                                               #\n","\t\t\t# Compute the l2 distance between the ith test point and all training #\n","\t\t\t# points, and store the result in dists[i, :].                        #\n","\t\t\t#######################################################################\n","\t\t\tdistance = np.array(list(map(lambda x: np.linalg.norm(x-X[i]), self.X_train)))\n","\t\t\tdists[i, :] = distance\n","\t\t\t#######################################################################\n","\t\t\t#                         END OF YOUR CODE                            #\n","\t\t\t#######################################################################\n","\t\treturn dists\n","\n","\tdef compute_distances_no_loops(self, X):\n","\t\t\"\"\"\n","\t\tCompute the distance between each test point in X and each training point\n","\t\tin self.X_train using no explicit loops.\n","\n","\t\tInput / Output: Same as compute_distances_two_loops\n","\t\t\"\"\"\n","\t\tnum_test = X.shape[0]\n","\t\tnum_train = self.X_train.shape[0]\n","\t\tdists = np.zeros((num_test, num_train)) \n","\t\t#########################################################################\n","\t\t# TODO:                                                                 #\n","\t\t# Compute the l2 distance between all test points and all training      #\n","\t\t# points without using any explicit loops, and store the result in      #\n","\t\t# dists.                                                                #\n","\t\t#                                                                       #\n","\t\t# You should implement this function using only basic array operations; #\n","\t\t# in particular you should not use functions from scipy.                #\n","\t\t#                                                                       #\n","\t\t# HINT: Try to formulate the l2 distance using matrix multiplication    #\n","\t\t#       and two broadcast sums.                                         #\n","\t\t#########################################################################\n","\t\ta, b = X.reshape(num_test, -1), self.X_train\n","\t\t\n","\t\ta2 = np.sum(a**2, axis=1, keepdims=True)\n","\t\tb2 = np.sum(b**2, axis=1)\n","\t\tab = np.dot(a, b.T)\n","\t\tdists = np.sqrt(a2 - 2*ab + b2)\n","\t\t#########################################################################\n","\t\t#                         END OF YOUR CODE                              #\n","\t\t#########################################################################\n","\t\treturn dists\n","\n","\tdef predict_labels(self, dists, k=1):\n","\t\t\"\"\"\n","\t\tGiven a matrix of distances between test points and training points,\n","\t\tpredict a label for each test point.\n","\n","\t\tInputs:\n","\t\t- dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n","\t\t\tgives the distance betwen the ith test point and the jth training point.\n","\n","\t\tReturns:\n","\t\t- y: A numpy array of shape (num_test,) containing predicted labels for the\n","\t\t\ttest data, where y[i] is the predicted label for the test point X[i].  \n","\t\t\"\"\"\n","\t\tnum_test = dists.shape[0]\n","\t\ty_pred = np.zeros(num_test)\n","\t\tfor i in range(num_test):\n","\t\t\t# A list of length k storing the labels of the k nearest neighbors to\n","\t\t\t# the ith test point.\n","\t\t\tclosest_y = []\n","\t\t\t#########################################################################\n","\t\t\t# TODO:                                                                 #\n","\t\t\t# Use the distance matrix to find the k nearest neighbors of the ith    #\n","\t\t\t# testing point, and use self.y_train to find the labels of these       #\n","\t\t\t# neighbors. Store these labels in closest_y.                           #\n","\t\t\t# Hint: Look up the function numpy.argsort.                             #\n","\t\t\t#########################################################################\n","\t\t\tclosest_y = list(np.argsort(dists[i]))[:k]\n","\t\t\t#########################################################################\n","\t\t\t# TODO:                                                                 #\n","\t\t\t# Now that you have found the labels of the k nearest neighbors, you    #\n","\t\t\t# need to find the most common label in the list closest_y of labels.   #\n","\t\t\t# Store this label in y_pred[i]. Break ties by choosing the smaller     #\n","\t\t\t# label.                                                                #\n","\t\t\t#########################################################################\n","\t\t\tlabels = list(self.y_train[closest_y])\n","\t\t\tres = dict((labels.count(j), j) for j in set(labels))\n","\t\t\ty_pred[i] = res[max(res.keys())]\n","\t\t\t#########################################################################\n","\t\t\t#                           END OF YOUR CODE                            # \n","\t\t\t#########################################################################\n","\n","\t\treturn y_pred\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgW7XALhEr52","colab_type":"text"},"source":["Linear svm"]},{"cell_type":"code","metadata":{"id":"g4DLMTAxEvpX","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Linear svm\n","import numpy as np\n","from random import shuffle\n","\n","\n","def svm_loss_naive(W, X, y, reg):\n","    \"\"\"\n","    Structured SVM loss function, naive implementation (with loops).\n","\n","    Inputs have dimension D, there are C classes, and we operate on minibatches\n","    of N examples.\n","\n","    Inputs:\n","    - W: A numpy array of shape (D, C) containing weights.\n","    - X: A numpy array of shape (N, D) containing a minibatch of data.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","      that X[i] has label c, where 0 <= c < C.\n","    - reg: (float) regularization strength\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    dW = np.zeros(W.shape)  # initialize the gradient as zero\n","\n","    # compute the loss and the gradient\n","    num_classes = W.shape[1]\n","    num_train = X.shape[0]\n","    loss = 0.0\n","\n","    for i in range(num_train):\n","        scores = X[i].dot(W)\n","        correct_class_score = scores[y[i]]\n","        for j in range(num_classes):\n","            if j == y[i]:\n","                continue\n","            margin = scores[j] - correct_class_score + 1  # note delta = 1\n","            if margin > 0:\n","                loss += margin\n","                dW[j, :] += X[:, i].T\n","                dW[y[i], :] -= X[:, i].T\n","\n","    # Right now the loss is a sum over all training examples, but we want it\n","    # to be an average instead so we divide by num_train.\n","    loss /= num_train\n","\n","    # Add regularization to the loss.\n","    loss += reg * np.sum(W * W)\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Compute the gradient of the loss function and store it dW.                #\n","    # Rather that first computing the loss and then computing the derivative,   #\n","    # it may be simpler to compute the derivative at the same time that the     #\n","    # loss is being computed. As a result you may need to modify some of the    #\n","    # code above to compute the gradient.                                       #\n","    #############################################################################\n","    \n","    return loss, dW\n","\n","\n","def svm_loss_vectorized(W, X, y, reg):\n","    \"\"\"\n","    Structured SVM loss function, vectorized implementation.\n","\n","    Inputs and outputs are the same as svm_loss_naive.\n","    \"\"\"\n","    loss = 0.0\n","    dW = np.zeros(W.shape)  # initialize the gradient as zero\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the structured SVM loss, storing the    #\n","    # result in loss.                                                           #\n","    #############################################################################\n","    num_classes = W.shape[1]\n","    num_train = X.shape[0]\n","    # correct_class_score = scores(y)\n","\n","\n","\n","    num_train = X.shape[0]\n","    scores = W.T.dot(X.T)\n","    sequence = np.array(range(num_train))\n","    correct_class_scores = scores[y, sequence]\n","\n","    margins = np.maximum(0, scores - correct_class_scores + 1)\n","    margins[y, sequence] = 0\n","    loss = np.sum(margins)\n","\n","    loss /= num_train\n","    loss += reg * np.sum(W * W)\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the gradient for the structured SVM     #\n","    # loss, storing the result in dW.                                           #\n","    #                                                                           #\n","    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n","    # to reuse some of the intermediate values that you used to compute the     #\n","    # loss.                                                                     #\n","    #############################################################################\n","    margins = np.where(margins > 0, 1, 0)\n","    margins[y, np.arange(0, scores.shape[1])] = -1 * np.sum(margins, 0)\n","    dW = np.dot(margins, X).T\n","    \n","    dW /= num_train\n","    dW += reg * W\n","\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","\n","    return loss, dW\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QfdI2ovE3y_","colab_type":"text"},"source":["softmax"]},{"cell_type":"code","metadata":{"id":"MZJNMYlOE5MX","colab_type":"code","cellView":"form","colab":{}},"source":["#@title softmax\n","import numpy as np\n","from random import shuffle\n","\n","\n","def softmax_loss_naive(W, X, y, reg):\n","    \"\"\"\n","    Softmax loss function, naive implementation (with loops)\n","\n","    Inputs have dimension D, there are C classes, and we operate on minibatches\n","    of N examples.\n","\n","    Inputs:\n","    - W: A numpy array of shape (D, C) containing weights.\n","    - X: A numpy array of shape (N, D) containing a minibatch of data.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","      that X[i] has label c, where 0 <= c < C.\n","    - reg: (float) regularization strength\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    #############################################################################\n","    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n","    # Store the loss in loss and the gradient in dW. If you are not careful     #\n","    # here, it is easy to run into numeric instability. Don't forget the        #\n","    # regularization!                                                           #\n","    #############################################################################\n","    num_dims = W.shape[0]\n","    num_classes = W.shape[1]\n","    num_train = X.shape[0]\n","\n","    l2_weights_norm = np.linalg.norm(W)\n","\n","    for i in range(num_train):\n","        scores = X[i, :].dot(W)\n","        exp_scores = np.exp(scores)\n","\n","        prob_scores = exp_scores/np.sum(exp_scores)\n","\n","        for d in range(num_dims):\n","            for k in range(num_classes):\n","                if k == y[i]:\n","                    dW[d, k] += X.T[d, i] * (prob_scores[k]-1)\n","                else:\n","                    dW[d, k] += X.T[d, i] * prob_scores[k]\n","\n","        loss += -np.log(prob_scores[y[i]])\n","\n","    loss /= num_train\n","    loss += reg * l2_weights_norm\n","\n","    dW /= num_train\n","    dW += reg * W\n","    #############################################################################\n","    #                          END OF YOUR CODE                                 #\n","    #############################################################################\n","\n","    return loss, dW\n","\n","\n","def softmax_loss_vectorized(W, X, y, reg):\n","    \"\"\"\n","    Softmax loss function, vectorized version.\n","\n","    Inputs and outputs are the same as softmax_loss_naive.\n","    \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    #############################################################################\n","    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n","    # Store the loss in loss and the gradient in dW. If you are not careful     #\n","    # here, it is easy to run into numeric instability. Don't forget the        #\n","    # regularization!                                                           #\n","    #############################################################################\n","    num_train = X.shape[0]\n","\n","    l2_weights_norm = np.linalg.norm(W)\n","\n","    scores = np.dot(X, W)\n","    exp_scores = np.exp(scores)\n","\n","    prob_scores = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n","    correct_log_probs = -np.log(prob_scores[range(num_train), y]+1e-10)\n","\n","    loss = np.sum(correct_log_probs)\n","    loss /= num_train\n","    loss += reg * l2_weights_norm\n","\n","    # grads\n","    dscores = prob_scores\n","    dscores[range(num_train), y] -= 1\n","    dW = np.dot(X.T, dscores)\n","    dW /= num_train\n","    dW += reg * W\n","    #############################################################################\n","    #                          END OF YOUR CODE                                 #\n","    #############################################################################\n","\n","    return loss, dW\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBJj6a5LFAI9","colab_type":"text"},"source":["Linear Classifier"]},{"cell_type":"code","metadata":{"id":"b6-WCy7nFCNz","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Linear Classifier\n","from __future__ import print_function\n","\n","import numpy as np\n","\n","class LinearClassifier(object):\n","\n","\tdef __init__(self):\n","\t\tself.W = None\n","\n","\tdef train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=500,\n","\t\t\t\t\t\tbatch_size=200, verbose=True):\n","\t\t\"\"\"\n","\t\tTrain this linear classifier using stochastic gradient descent.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (N, D) containing training data; there are N\n","\t\t\ttraining samples each of dimension D.\n","\t\t- y: A numpy array of shape (N,) containing training labels; y[i] = c\n","\t\t\tmeans that X[i] has label 0 <= c < C for C classes.\n","\t\t- learning_rate: (float) learning rate for optimization.\n","\t\t- reg: (float) regularization strength.\n","\t\t- num_iters: (integer) number of steps to take when optimizing\n","\t\t- batch_size: (integer) number of training examples to use at each step.\n","\t\t- verbose: (boolean) If true, print progress during optimization.\n","\n","\t\tOutputs:\n","\t\tA list containing the value of the loss function at each training iteration.\n","\t\t\"\"\"\n","\t\tnum_train, dim = X.shape\n","\t\tnum_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","\t\tif self.W is None:\n","\t\t\t# lazily initialize W\n","\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","\t\t# Run stochastic gradient descent to optimize W\n","\t\tloss_history = []\n","\t\tindeces = np.arange(num_train)\n","\t\tfor it in range(num_iters):\n","\t\t\tX_batch = None\n","\t\t\ty_batch = None\n","\n","\t\t\t#########################################################################\n","\t\t\t# TODO:                                                                 #\n","\t\t\t# Sample batch_size elements from the training data and their           #\n","\t\t\t# corresponding labels to use in this round of gradient descent.        #\n","\t\t\t# Store the data in X_batch and their corresponding labels in           #\n","\t\t\t# y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n","\t\t\t# and y_batch should have shape (batch_size,)                           #\n","\t\t\t#                                                                       #\n","\t\t\t# Hint: Use np.random.choice to generate indices. Sampling with         #\n","\t\t\t# replacement is faster than sampling without replacement.              #\n","\t\t\t#########################################################################\n","\t\t\tbatch_idxs = np.random.choice(indeces, batch_size)\n","\t\t\tX_batch = X[batch_idxs]\n","\t\t\ty_batch = y[batch_idxs]\n","\t\t\t#########################################################################\n","\t\t\t#                       END OF YOUR CODE                                #\n","\t\t\t#########################################################################\n","\n","\t\t\t# evaluate loss and gradient\n","\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n","\t\t\tloss_history.append(loss)\n","\n","\t\t\t# perform parameter update\n","\t\t\t#########################################################################\n","\t\t\t# TODO:                                                                 #\n","\t\t\t# Update the weights using the gradient and the learning rate.          #\n","\t\t\t#########################################################################\n","\t\t\tself.W = self.W - learning_rate*grad\n","\n","\t\t\t# idxs = np.arange(len(y_batch))\n","\t\t\t# np.random.shuffle(idxs)\n","\t\t\t# X[:] = X[idxs]\n","\t\t\t# y[:] = y[idxs]\n","\t\t\t#########################################################################\n","\t\t\t#                       END OF YOUR CODE                                #\n","\t\t\t#########################################################################\n","\n","\t\t\tif verbose and it % 100 == 0:\n","\t\t\t\tprint('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","\t\treturn loss_history\n","\n","\tdef predict(self, X):\n","\t\t\"\"\"\n","\t\tUse the trained weights of this linear classifier to predict labels for\n","\t\tdata points.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (N, D) containing training data; there are N\n","\t\t\ttraining samples each of dimension D.\n","\n","\t\tReturns:\n","\t\t- y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","\t\t\tarray of length N, and each element is an integer giving the predicted\n","\t\t\tclass.\n","\t\t\"\"\"\n","\t\ty_pred = np.zeros(X.shape[0])\n","\t\t###########################################################################\n","\t\t# TODO:                                                                   #\n","\t\t# Implement this method. Store the predicted labels in y_pred.            #\n","\t\t###########################################################################\n","\t\ty_pred = self.W.T.dot(X.T)\n","\t\t###########################################################################\n","\t\t#                           END OF YOUR CODE                              #\n","\t\t###########################################################################\n","\t\treturn y_pred\n","\t\n","\tdef loss(self, X_batch, y_batch, reg):\n","\t\t\"\"\"\n","\t\tCompute the loss function and its derivative. \n","\t\tSubclasses will override this.\n","\n","\t\tInputs:\n","\t\t- X_batch: A numpy array of shape (N, D) containing a minibatch of N\n","\t\t\tdata points; each point has dimension D.\n","\t\t- y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n","\t\t- reg: (float) regularization strength.\n","\n","\t\tReturns: A tuple containing:\n","\t\t- loss as a single float\n","\t\t- gradient with respect to self.W; an array of the same shape as W\n","\t\t\"\"\"\n","\n","\t\tpass\n","\n","\n","class LinearSVM(LinearClassifier):\n","\t\"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n","\n","\tdef loss(self, X_batch, y_batch, reg):\n","\t\treturn svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n","\n","\n","class Softmax(LinearClassifier):\n","\t\"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n","\n","\tdef loss(self, X_batch, y_batch, reg):\n","\t\treturn softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwtoVcSjQZO2","colab_type":"text"},"source":["NN"]},{"cell_type":"code","metadata":{"id":"hAK_kGzbQYcY","colab_type":"code","cellView":"form","colab":{}},"source":["#@title NN\n","\n","from __future__ import print_function\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","class TwoLayerNet(object):\n","\t\"\"\"\n","\tA two-layer fully-connected neural network. The net has an input dimension of\n","\tN, a hidden layer dimension of H, and performs classification over C classes.\n","\tWe train the network with a softmax loss function and L2 regularization on the\n","\tweight matrices. The network uses a ReLU nonlinearity after the first fully\n","\tconnected layer.\n","\n","\tIn other words, the network has the following architecture:\n","\n","\tinput - fully connected layer - ReLU - fully connected layer - softmax\n","\n","\tThe outputs of the second fully-connected layer are the scores for each class.\n","\t\"\"\"\n","\n","\tdef __init__(self, input_size, hidden_size, output_size, std=1e-4):\n","\t\t\"\"\"\n","\t\tInitialize the model. Weights are initialized to small random values and\n","\t\tbiases are initialized to zero. Weights and biases are stored in the\n","\t\tvariable self.params, which is a dictionary with the following keys:\n","\n","\t\tW1: First layer weights; has shape (D, H)\n","\t\tb1: First layer biases; has shape (H,)\n","\t\tW2: Second layer weights; has shape (H, C)\n","\t\tb2: Second layer biases; has shape (C,)\n","\n","\t\tInputs:\n","\t\t- input_size: The dimension D of the input data.\n","\t\t- hidden_size: The number of neurons H in the hidden layer.\n","\t\t- output_size: The number of classes C.\n","\t\t\"\"\"\n","\t\tself.params = {}\n","\t\tself.params['W1'] = std * np.random.randn(input_size, hidden_size)\n","\t\tself.params['b1'] = np.zeros(hidden_size)\n","\t\tself.params['W2'] = std * np.random.randn(hidden_size, output_size)\n","\t\tself.params['b2'] = np.zeros(output_size)\n","\n","\tdef loss(self, X, y=None, reg=0.0):\n","\t\t\"\"\"\n","\t\tCompute the loss and gradients for a two layer fully connected neural\n","\t\tnetwork.\n","\n","\t\tInputs:\n","\t\t- X: Input data of shape (N, D). Each X[i] is a training sample.\n","\t\t- y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n","\t\t  an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n","\t\t  is not passed then we only return scores, and if it is passed then we\n","\t\t  instead return the loss and gradients.\n","\t\t- reg: Regularization strength.\n","\n","\t\tReturns:\n","\t\tIf y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n","\t\tthe score for class c on input X[i].\n","\n","\t\tIf y is not None, instead return a tuple of:\n","\t\t- loss: Loss (data loss and regularization loss) for this batch of training\n","\t\t  samples.\n","\t\t- grads: Dictionary mapping parameter names to gradients of those parameters\n","\t\t  with respect to the loss function; has the same keys as self.params.\n","\t\t\"\"\"\n","\t\t# Unpack variables from the params dictionary\n","\t\tW1, b1 = self.params['W1'], self.params['b1']\n","\t\tW2, b2 = self.params['W2'], self.params['b2']\n","\t\tN, D = X.shape\n","\n","\t\t# Compute the forward pass\n","\t\tscores = None\n","\t\t#############################################################################\n","\t\t# TODO: Perform the forward pass, computing the class scores for the input. #\n","\t\t# Store the result in the scores variable, which should be an array of      #\n","\t\t# shape (N, C).                                                             #\n","\t\t#############################################################################\n","\t\tl1 = X.dot(W1) + b1\n","\t\tl1 = np.dot(X, W1) + b1\n","\t\tl1[l1 <= 0] = 0  # ReLu\n","\t\tscores = l1.dot(W2) + b2\n","\t\t#############################################################################\n","\t\t#                              END OF YOUR CODE                             #\n","\t\t#############################################################################\n","\n","\t\t# If the targets are not given then jump out, we're done\n","\t\tif y is None:\n","\t\t\treturn scores\n","\n","\t\t# Compute the loss\n","\t\tloss = None\n","\t\t#############################################################################\n","\t\t# TODO: Finish the forward pass, and compute the loss. This should include  #\n","\t\t# both the data loss and L2 regularization for W1 and W2. Store the result  #\n","\t\t# in the variable loss, which should be a scalar. Use the Softmax           #\n","\t\t# classifier loss.                                                          #\n","\t\t#############################################################################\n","\n","\t\ttmp = -np.log(np.exp(scores[range(len(y)), y])/np.sum(np.exp(scores), axis=1))\n","\t\tloss = np.sum(tmp)/len(y) + reg*0.5* np.sum(W1**2) + reg*0.5* np.sum(W2**2)\n","\n","\t\t#############################################################################\n","\t\t#                              END OF YOUR CODE                             #\n","\t\t#############################################################################\n","\n","\t\t# Backward pass: compute gradients\n","\t\tgrads = {}\n","\t\t#############################################################################\n","\t\t# TODO: Compute the backward pass, computing the derivatives of the weights #\n","\t\t# and biases. Store the results in the grads dictionary. For example,       #\n","\t\t# grads['W1'] should store the gradient on W1, and be a matrix of same size #\n","\t\t#############################################################################\n","\t\tprobs = np.exp(scores)/np.sum(np.exp(scores), keepdims=True, axis=1)\n","\t\tdscores = probs\n","\t\tdscores[range(len(y)), y] -= 1\n","\t\tdscores /= len(y)\n","\n","\t\tgrads['W2'] = np.dot(l1.T, dscores)\n","\t\tgrads['W2'] += reg*W2\n","\t\tgrads['b2'] = np.sum(dscores, axis=0)\n","\n","\t\tdl1 = np.dot(dscores, W2.T)\n","\t\tdl1[l1 <= 0] = 0\n","\t\tgrads['W1'] = np.dot(X.T, dl1)\n","\t\tgrads['W1'] += reg*W1\n","\t\tgrads['b1'] = np.sum(dl1, axis=0)\n","\t\t#############################################################################\n","\t\t#                              END OF YOUR CODE                             #\n","\t\t#############################################################################\n","\t\treturn loss, grads\n","\n","\tdef train(self, X, y, X_val, y_val,\n","\t\t\t  learning_rate=1e-3, learning_rate_decay=0.95,\n","\t\t\t  reg=5e-6, num_iters=100,\n","\t\t\t  batch_size=200, verbose=True):\n","\t\t\"\"\"\n","\t\tTrain this neural network using stochastic gradient descent.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (N, D) giving training data.\n","\t\t- y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n","\t\t  X[i] has label c, where 0 <= c < C.\n","\t\t- X_val: A numpy array of shape (N_val, D) giving validation data.\n","\t\t- y_val: A numpy array of shape (N_val,) giving validation labels.\n","\t\t- learning_rate: Scalar giving learning rate for optimization.\n","\t\t- learning_rate_decay: Scalar giving factor used to decay the learning rate\n","\t\t  after each epoch.\n","\t\t- reg: Scalar giving regularization strength.\n","\t\t- num_iters: Number of steps to take when optimizing.\n","\t\t- batch_size: Number of training examples to use per step.\n","\t\t- verbose: boolean; if true print progress during optimization.\n","\t\t\"\"\"\n","\t\tnum_train = X.shape[0]\n","\t\titerations_per_epoch = max(num_train / batch_size, 1)\n","\n","\t\t# Use SGD to optimize the parameters in self.model\n","\t\tloss_history = []\n","\t\ttrain_acc_history = []\n","\t\tval_acc_history = []\n","\n","\t\tindeces = np.arange(num_train)\n","\n","\t\tfor it in range(num_iters):\n","\t\t\tX_batch = None\n","\t\t\ty_batch = None\n","\n","\t\t\t#########################################################################\n","\t\t\t# TODO: Create a random minibatch of training data and labels, storing  #\n","\t\t\t# them in X_batch and y_batch respectively.                             #\n","\t\t\t#########################################################################\n","\t\t\tbatch_idxs = np.random.choice(indeces, batch_size)\n","\t\t\tX_batch = X[batch_idxs]\n","\t\t\ty_batch = y[batch_idxs]\n","\t\t\t#########################################################################\n","\t\t\t#                             END OF YOUR CODE                          #\n","\t\t\t#########################################################################\n","\n","\t\t\t# Compute loss and gradients using the current minibatch\n","\t\t\tloss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n","\t\t\tloss_history.append(loss)\n","\n","\t\t\t#########################################################################\n","\t\t\t# TODO: Use the gradients in the grads dictionary to update the         #\n","\t\t\t# parameters of the network (stored in the dictionary self.params)      #\n","\t\t\t# using stochastic gradient descent. You'll need to use the gradients   #\n","\t\t\t# stored in the grads dictionary defined above.                         #\n","\t\t\t#########################################################################\n","\t\t\tself.params['W1'] += -learning_rate*grads['W1']\n","\t\t\tself.params['W2'] += -learning_rate*grads['W2']\n","\t\t\tself.params['b1'] += -learning_rate*grads['b1']\n","\t\t\tself.params['b2'] += -learning_rate*grads['b2']\n","\t\t\t#########################################################################\n","\t\t\t#                             END OF YOUR CODE                          #\n","\t\t\t#########################################################################\n","\n","\t\t\tif verbose and it % 100 == 0:\n","\t\t\t\tprint('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","\t\t\t# Every epoch, check train and val accuracy and decay learning rate.\n","\t\t\tif it % iterations_per_epoch == 0:\n","\t\t\t\t# Check accuracy\n","\t\t\t\ttrain_acc = (self.predict(X_batch) == y_batch).mean()\n","\t\t\t\tval_acc = (self.predict(X_val) == y_val).mean()\n","\t\t\t\ttrain_acc_history.append(train_acc)\n","\t\t\t\tval_acc_history.append(val_acc)\n","\n","\t\t\t\t# Decay learning rate\n","\t\t\t\tlearning_rate *= learning_rate_decay\n","\n","\t\treturn {\n","\t\t\t'loss_history': loss_history,\n","\t\t\t'train_acc_history': train_acc_history,\n","\t\t\t'val_acc_history': val_acc_history,\n","\t\t}\n","\n","\tdef predict(self, X):\n","\t\t\"\"\"\n","\t\tUse the trained weights of this two-layer network to predict labels for\n","\t\tdata points. For each data point we predict scores for each of the C\n","\t\tclasses, and assign each data point to the class with the highest score.\n","\n","\t\tInputs:\n","\t\t- X: A numpy array of shape (N, D) giving N D-dimensional data points to\n","\t\t  classify.\n","\n","\t\tReturns:\n","\t\t- y_pred: A numpy array of shape (N,) giving predicted labels for each of\n","\t\t  the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n","\t\t  to have class c, where 0 <= c < C.\n","\t\t\"\"\"\n","\t\ty_pred = None\n","\n","\t\t###########################################################################\n","\t\t# TODO: Implement this function; it should be VERY simple!                #\n","\t\t###########################################################################\n","\t\tl1 = X.dot(self.params['W1']) + self.params['b1']\n","\t\tl1[l1 <= 0] = 0  # ReLu\n","\t\tscores = l1.dot(self.params['W2']) + self.params['b2']\n","\t\ty_pred = np.argmax(scores, axis=1)\n","\t\t###########################################################################\n","\t\t#                              END OF YOUR CODE                           #\n","\t\t###########################################################################\n","\n","\t\treturn y_pred\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C19VaXqj10Go","colab_type":"text"},"source":["# Image features exercise\n"]},{"cell_type":"code","metadata":{"id":"PoAk4iBN10Gq","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading extenrnal modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","# %autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi3o_XbX10Gx","colab_type":"text"},"source":["## Load data\n","Similar to previous exercises, we will load CIFAR-10 data from disk."]},{"cell_type":"code","metadata":{"id":"9C1II4T510Gz","colab_type":"code","outputId":"4513efc4-4bbc-496c-91c9-ba5932b48712","executionInfo":{"status":"ok","timestamp":1582656267721,"user_tz":-240,"elapsed":1890,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# from cs231n.features import color_histogram_hsv, hog_feature\n","\n","def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cifar-10-batches-py'\n","\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","    \n","    # Subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","    \n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","try:\n","   del X_train, y_train\n","   del X_test, y_test\n","   print('Clear previously loaded data.')\n","except:\n","   pass\n","\n","X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()"],"execution_count":125,"outputs":[{"output_type":"stream","text":["Clear previously loaded data.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"16rPn1SO10G2","colab_type":"text"},"source":["## Extract Features and train NN model\n","Find any feature extractor for every image and run neural network classifier on it"]},{"cell_type":"code","metadata":{"id":"cR-mfTNYpOO1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":49},"outputId":"da927d5e-c842-41f4-cdd1-2068d8c90254","executionInfo":{"status":"ok","timestamp":1582656272323,"user_tz":-240,"elapsed":728,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}}},"source":["from PIL import Image\n","\n","Image.fromarray(X_test[int(np.random.rand()*100)].astype('uint8'))"],"execution_count":128,"outputs":[{"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJuklEQVR4nAXBWW9c52EA0G+/69w7\ndxYOKS6iRZGULDuW7NgwWvghaYG8FH3pU5H0B/SnFS2QIECBwG5gIEGLuqYiKZIpURTX4Qw5253l\nrt/ec+A3n3SFVthhaxtrYeS6ge8FvXtbB2EYLoczRryHh/vD06PQZcyLDCSHWzuTu9v+fDLMUuhF\n//Lrf533h4Obod9onL9/s8yW//WnPzHKWs1Wu9Nb1pxoY4njYkYpZQQ7Zam0LdL5eDTqk0oYrkKn\n3moaLbLVojq7nI3fnOaLNNxZxwRtbm72er2mF+dlPZ/duqCeM/XlL76p+6vr635jvx1SRhBhhDkW\no3S+NAYIDaTKx7NXzUbj2f7H48Gd0GqMWimvhQ3B1kfKgbf1q2bFrq+ulgs8/eXsdjhN8/zlX35o\nmLzPy3hzvx21iiibzWYsbBDMXGmslAIjyyvthXEtl1LX60mSZdWitCBDaU259lzqJXEHd9yvHx9c\n/3QyffEhL2/+7d//4+dfffPD8yNezpNW0GW06QWfPXn2aLZ3fP7ux7++JlzIWitEsLVWKompCXw3\nYKQdJXsPn5RmSOOt+e2C82yrC77+dHur3dxoty7bycnJm6vLixcvXrOg++bt8d4G+/jZF0CiMOp8\n+sVTCC3/3fLk/TEpawEgAhZJLRzXsVBhBNqNZq/RItCELmh7Zl6PhdLy8hrsBqH7mSQooCCO28yv\nDQg8x2k2/NvRnSKJlAoaiBuBH4ROHH76+R7CmFDsuNRz3ZA6jrGyrsoyywfXFz8d/wjk+OOtYI2U\nHyXueuievjiKI6+33puP02I6rZaz4e21NlXUSXJD/vPb73//3R9fnp7NsgpAfP/hYcEFMcpYIJFF\n2CEEIkZQ4HntdmtjrYs8ukhTa3KA9KtXzw8PD5rdtXTaL2vVP+2vRXr/7+5Z6lfZORd5uspnw//J\nVvlVv797/2D7n/4RI3d/73NilMKIVmXFNBHCkjjIFqsJtPfWYlkWq9WoKqYHD3ePfjgaD65Esbzp\nf4jjXW78gPGvHrUppoVSl8dTk41FJaxWZZZdXVzVlZiny8+f/Q0RNbdWCKEpJ0rzMl8FAes0G14Q\nhAy6SCmeN93eL7/+MsvzRSm4VePpBfGTzR6MCQO8dJnniUyl41JAA2mZL2/659dXZ6tsOZvNiRJS\nSm01pAQZrUWto9AlhJ6d9+8/uOe5PtBWVcVer1c0wpt5PZrllajuJqeR2xV1Mk5Nmi+WBeaS5KuS\neUhrfnX59tvvfn95fl1VNfECt57nFgAhJUSIMAawM5rOa1nvH94XqwWjqO16opDLbFkUhZB6cDdK\neX4z9Y5Opn95+36aLpSFhgTYBXE7acTBbD56/uPR+G7S7SbEQsMcIoTBBGGKvDBAzCmFbCLvdjhU\n+WJ/fStsoMb69osPlz+dvKNeUPGs4opDd1KZs8FkPJ74vhc2IkMQYsQL/TSdQoOfHD767OkBEVIi\njCiFrku1rYHlWkOEsFL67PJu3h/st/Y+2XzEwgaizmS22N6LqQPksro4v+lf9SfzsZSSECyVnKdp\nkZOyWDKHnp69z7rJRo8SJRVGlBAKrIpCBrDFSLkOLatK5KrOgK5QM+mmeXXRHwLqZlUlROVgMBnP\nsnylbIExNsYQQjCEknNFEQb2fHSqxcZi0SXWGgCh53lRgNfX46KqlmUBAZwvskUBO4D1Wu0gTn66\nuj35cGEQFRpBiJGpgEXSaGO01ppzEUUNSoiUKvQ8hIDvuFKI6Twn2hhkjBAChSGBTMsSalTnfLEo\nFhw+etBb6wTKWi/pbO48oCsZdzqCi5oUSEkLtQUAAmCtLYoicF1rTJEVrsesskUu7tIaEUyssVrp\n5bI6fd+fzVYA4KpWQlmlQNyIHAxuz9734sbffvl0v9e0WWqU1NYSTDzHAwZaCwGAWhkuFXMcBGFd\nlGWR53l1enpDrIVSKEpxqRQE2gmhLCuhDKIOKlWDBZary9cvSTI8WG85qf+GT/syW5VV0trifFGA\nFQQAAAAgEEo5mlgNgJTaSFlZNRyhsuBaAwBgXdcWGcwwl1wISSBsB97He/c9iqs8e/ni+Xw2eXzw\n8Ge7O6FVTd+hDEMMEcIQIowxRggBoISoKw4ghphAiJRSiBAKANTKIoK55NpaiClXVlSyHXiP9+53\nOi0IYV3Vb9+941ysReFXj/Z3N3pScEQwxghCaK211iKErUXSoEpCi1m7u5YkTQIsNBpIaxijmDra\nYgsIwsz1/MAPZpPJDEVxM8GD8XA4HKytOUpJXiOAHCegHECIEAJaa4iwRUQhAqnDlUHQBI0YaEWq\nWiBErDU1ly5hShFEQsdzDcEQ09Ozyyba7sbxzs52wS/zLCsEB5RBaiWXWiJMsFYGWAsAdMI2dgLM\n3F6nd9d/l5VVK/QJAghYTIgXxokTuJgiACikoTTKWtPs9vxWd/1ey0k6xAkSv1FzjktpZwITh2Lp\nYJoJaRAGkCA38prdWpiou+M4ZHLx2un4pBEl1G2GUQcSVtcllLoVNzDBs+VyOJi8ftfoJvH9j3a6\njn912f/uh6O0qH/+yaENOrPXgyxbWAE0cKzjAkCFocxihMhoOHqws12n01prtP3kZ9uHB07k17xk\nGLXiqB03eJkt0pmQ+vmrN3/+3+eD2SpodtY3thZF/dfjk8f7Dx8fHDAvWAmdW2bdJnGTMGg3XT+A\nyAUgIMRnzPd9rRXptDZkVTQitLexJrjkVZXnuQWwmXSxlqYunr85DaLv13/zz3Fns9Hs7m7X7WZz\nyivmhsBp0FbgOMzDzv7G1t5mzwAbd9YeHD767vtvV8s0aSDiFrzNqEMI8tyT0WSV5YQxP2r5ECJj\njOAModFS/ffR8aO93UqCdFl8OO+/Oru7vRk0fL/phxaBiDr/8Pe/eHbwICvyuLd+MRxefDim2CJg\nSOLB0XigDQ86W4C4NCSIutwYhgB1mR91wyC4t9FNC31ydtNs9bb28GTJGfOePj6klCHqSKDu3+t9\ndrAVO8hI9Pb45W//8Ic0veu2fEYx/NUXn746PqYe3d176gfrXNtawzzPRDbrJG3P8aOoaa18uJY8\nO3wwzVYcOY/Xm0EUFTi0tVjOl5nIANBUa5/6ZzeD7//vx+vJWAHVDKmDLVksFkBhrGg2n81uxhBi\nTeiizKziPtE8g7KcTudpNWuV8xsGdVrbd6La2O5czXOr3HQ0SEJaYj90mMvw1SidzHNigeCywsQ6\nmMRhuN6yvutURjpYJV6orUZcKAxcWK/ShVwRSOhqNbuoF1vtOF0Wti6IW1wO7lYp6IZs//GTN8N8\nOrlbLGcrRYibmLKyymDMMKX/D5/N1xzqlFZnAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=RGB size=32x32 at 0x7F8E80661128>"]},"metadata":{"tags":[]},"execution_count":128}]},{"cell_type":"code","metadata":{"id":"zJ6u4ZvG10G4","colab_type":"code","outputId":"ac554c04-0911-46a9-8ab7-22bcda6c3fae","executionInfo":{"status":"ok","timestamp":1582653679109,"user_tz":-240,"elapsed":19655,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_train.shape"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49000, 32, 32, 3)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Z3IdN0L-fY8q","colab_type":"code","outputId":"22dcfb08-901a-4f6f-f055-b3c740d8988d","executionInfo":{"status":"ok","timestamp":1582653679110,"user_tz":-240,"elapsed":19382,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_train[0].reshape(-1).shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3072,)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"PHMqtqYHPe_-","colab_type":"code","colab":{}},"source":["def pipeline(X_train, y_train, X_val, y_val, X_test, y_test):\n","    X_train, X_val, X_test = X_train.reshape(X_train.shape[0], -1), X_val.reshape(X_val.shape[0], -1), X_test.reshape(X_test.shape[0], -1)\n","    # X_train, X_val, X_test = X_train / 255, X_val / 255, X_test / 255\n","    mean = np.mean(X_train, axis=0)\n","    std = np.std(X_train, axis=0)\n","    X_train, X_val, X_test = (X_train - mean) / std, (X_val - mean) / std, (X_test - mean) / std \n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","X_train, y_train, X_val, y_val, X_test, y_test = pipeline(X_train, y_train, X_val, y_val, X_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAS4xSCkP9IN","colab_type":"code","colab":{}},"source":["model = KNearestNeighbor()\n","model = LinearSVM()\n","# model = Softmax()\n","# model = TwoLayerNet(3072, 1024, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhYRF7xsHxDe","colab_type":"code","colab":{}},"source":["def train(model, X_train, y_train, **kwargs):\n","\n","    def calculate_acc(predicts, y):\n","        acc = 0.0\n","        acc_tmp = 0.0\n","        loss = 0.0\n","        if predicts.ndim > 1:\n","            predicts = predicts.T\n","\n","            for i, pred in enumerate(predicts):\n","                if np.argmax(pred) == y[i]:\n","                    acc_tmp += 1\n","        else:\n","            for i, pred in enumerate(predicts):\n","                if pred == y[i]:\n","                    acc_tmp += 1\n","\n","        acc = acc_tmp / len(y)\n","\n","        return acc\n","\n","    run_data = []\n","    \n","    try:\n","        hist = model.train(\n","            X_train,\n","            y_train,\n","            X_val=kwargs['X_val'],\n","            y_val=kwargs['y_val'],\n","            learning_rate=kwargs['learning_rate'],\n","            reg=kwargs['reg'],\n","            num_iters=kwargs['num_iters'],\n","            batch_size=kwargs['batch_size'],\n","            verbose=kwargs['verbose']\n","            )\n","    except TypeError:\n","        hist = model.train(X_train, y_train)\n","\n","    results = {}\n","\n","    if hist is not None:\n","        if type(hist) == dict:\n","            results['train_acc'] = hist['train_acc_history'][-1]\n","            results['train_loss'] = hist['loss_history'][-1]\n","\n","            results['vall_acc'] = hist['val_acc_history'][-1]\n","        else:\n","            results['train_acc'] = calculate_acc(model.predict(X_train), y_train)\n","            results['train_loss'] = hist[-1]\n","\n","            results['vall_acc'] = calculate_acc(model.predict(X_val), y_val)\n","    else:\n","        results['train_acc'] = calculate_acc(model.predict(X_train), y_train)\n","        results['vall_acc'] = calculate_acc(model.predict(X_val), y_val)\n","\n","    run_data.append(results)\n","    df = pd.DataFrame.from_dict(run_data, orient = 'columns')\n","    display(df)\n","\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVhrjZZbFG2K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"ef30478b-650f-4c51-9f73-6a7c21b4b68c","executionInfo":{"status":"ok","timestamp":1582656017867,"user_tz":-240,"elapsed":4251,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}}},"source":["n_samples = len(X_train)\n","# use this for KNN\n","# n_samples = 10000\n","\n","model = train(\n","    model, \n","    X_train[:n_samples], \n","    y_train[:n_samples],\n","    X_val=X_val[:n_samples], \n","    y_val=y_val[:n_samples],\n","    learning_rate=0.01,\n","    reg=1e-4,\n","    num_iters=600,\n","    batch_size=200,\n","    verbose=True\n",")"],"execution_count":119,"outputs":[{"output_type":"stream","text":["iteration 0 / 500: loss 8.983076\n","iteration 100 / 500: loss 4.991880\n","iteration 200 / 500: loss 4.235954\n","iteration 300 / 500: loss 4.790453\n","iteration 400 / 500: loss 4.694735\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_acc</th>\n","      <th>train_loss</th>\n","      <th>vall_acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.392735</td>\n","      <td>3.880905</td>\n","      <td>0.397</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_acc  train_loss  vall_acc\n","0   0.392735    3.880905     0.397"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Asm96Igw10G7","colab_type":"text"},"source":["# Evaluate model"]},{"cell_type":"code","metadata":{"id":"UAHWLud4jgpL","colab_type":"code","colab":{}},"source":["def test_per_class(model, x, y):\n","    acc_per_class = {}\n","    predicts = model.predict(x)\n","\n","    if predicts.ndim > 1:\n","        predicts = predicts.T\n","        predicts = np.argmax(predicts, axis=1)\n","\n","    for cls in set(y):\n","        tmp = (y==cls)\n","        acc = (predicts[tmp]==y[tmp]).mean()\n","        acc_per_class[str(cls)] = acc\n","        print('class {0} accuracy {1:.3}%'.format(cls, acc*100))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGaszrmSkhyT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"7366930a-8bd1-4bf4-bc59-357fd136d1d4","executionInfo":{"status":"ok","timestamp":1582656022639,"user_tz":-240,"elapsed":907,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}}},"source":["test_per_class(model, X_val, y_val)"],"execution_count":121,"outputs":[{"output_type":"stream","text":["class 0 accuracy 33.3%\n","class 1 accuracy 44.5%\n","class 2 accuracy 21.5%\n","class 3 accuracy 33.0%\n","class 4 accuracy 32.1%\n","class 5 accuracy 20.4%\n","class 6 accuracy 50.0%\n","class 7 accuracy 39.3%\n","class 8 accuracy 69.9%\n","class 9 accuracy 41.9%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eIvpAv2qh71o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":49},"outputId":"20837bad-5485-4d07-e443-5b1df03cad6d","executionInfo":{"status":"ok","timestamp":1582655912476,"user_tz":-240,"elapsed":682,"user":{"displayName":"Baka F","photoUrl":"","userId":"05022197215042441803"}}},"source":[""],"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJS0lEQVR4nC3W6Y+d91XA8XPOb3mW\nu9+Z8UzszIxjxyR2tkmJVQWqBigiFQ20lUCiAvGfob4CCTVxU4IEKICjEpWUbLiu7djNeDyLZ5+5\nM/e5z/ZbzuFF+RO+rz5fXHt9bTQ3N7cwnw5SMnE8GA27F955+50Xb7woAMhSzIpZVQnzrCxnVVnW\nVVGWk7OzhdH48tKl6AMZFZlb56y1Wimj9WefffrjH//drCiCD/rx1+v0eF1p6s535y8Orr94Y3F0\ncWf36e7J4XRWRO8n0/OyqrRSTd3Udc0gSASEnSRdWlq02ihrkAgBSCkiMlrfufO/m5ubVVUqVHp4\nsWNBBedWn124+tLycJDHprh3924y7M2aCgWRUJjHw9GbN7/ZS/PE2jTPH339m729vX5/YLUWQkUk\nIsICCEQUQhARIgIQfXXtUgppfV7PLQ6crY7qtpsv/OC7f6E6+U/ev3V8dFzOinJafOPVtZfe+X4/\n7woIKvrpe+8dHB/Jn/+AIwsgAiIAAIAAiMQYY4xN0yginWY6U9ax5xTQmLLwtQ/j0Wj/6PjRvQc+\nhNC00bnFufnEWB8DEU0nZw/u3+/1e3VdqU4uUUIbCMRaw6QQCJEIlSaTZVajV2mWuR4tzF/85s03\nv7q3DrUEH3KbvLH2OmpFUYxSN2/eVEZHH0REEb3zp9/TxgAhED386quvfrP+8vXfuf7CCwJABG1b\nR+HUWmutTvVoPHhmZOzi3LMmXlhe0KcHW21br16+/Ld//TeMAJGN0mmacowAEGNUSr311luzpvn8\nyzt7u/vFrJi1RdWeb20/mR/Pp9bu7z9NUiMeiVCfT/ylxazbGSGruqqFA8fWudZorU1XBDgEIiIi\nEAFEIhIRRNze3vrZ++/v7OyNR+Nr15b7Sb7QHz9df9wGT4jWGlFKmPVksnt0lJzM9l3TPHflUidN\nG1c750VAmAlRBIRFhEUECIlUWZa/+O9f3Lr1k/WvHxuTTY9PLYTfu/nmtWs3Dp7uAZAmk2YdrZRv\nav3SjeXBOC1CjWnAdFqHwgVqXY0IwtzUTYicJIlOEgFE4UePHnzwT/98+/btyem+0qapao5wfj4M\nQv9w61ZxdtxLO3fu3Ot3h0phEVkPdS8xneOmMDk4DtByiMJQt0318c9v37/7ax/j3Ki/urI6GF/Y\n3dn68F//7dGj9aZpFYtzDen0jZvfAPAnR4cf/OyDt7/3J7t7R+sbm/1BJ8+Tuqz1Qn9paeXSg48e\nSjJz5ZEllZuu5+qj27ff/ce/n5ycurLoWjREZdVOZm1k6HZ7mSYXFaBavvLcX/3oRx9//NFnn396\ndHS0v7f38MGjsix9aJJkiYh0K+iigJAvQzTYGo9Em9tPfv3JxuT4uCzLtqyj4n6alOfnxbROs7yT\njJMkOT0PDPDH3/nOlatX37317v7ecb/f+fzTTyenZ84557ipW9c6vXW465KWtMQGXAPEojU93th4\neP/+2fGxi8E7bqNLk0xsJ4QZc0isyjM7LWj5mWe+9fvfStJ0be13wyvx1q2fbjxetzYV4cFgIEBt\n2+rcwtnBzji3c8NLqGOE1phka3Nze3NTQ0i6ua9LlNi6tqwbIXDenU3PbKKyJLl29fm5ubF3/tvf\n/oNf3b2zv7cbQ2AdbWLTNOUQQ4h6fsF6L0GUTXK0MmvOq9YfPD2sm7qbKqPUTJhZzoqiqlofI6PM\nqmo+jJi5NxhEBt9479zOzk5V19ZaBjFaHR7u51lujNEuc5CSAgvgIwthr5N0WjcFUggUPSR5X1tV\nFWXRVm0btTW2bk/PZ4ElKjqZnv/y40+++PKLre0tEUGkXt4ZjUb3798XZqWMhkREOIQQiSTq06Pq\nydebuweHaZ5LaBcWl777w79cvby6v/f03t27p6eT7ac72082JlWrE/s/X3z5eG//4YNHRwcHrm0R\nMcaQaasFgcU5h+i1ISBSolXErK4aUG3WjUgAgmnHHk4P/+uXn+yfTi6vrP7R23+WGPUf//nh4eFR\n452I1Fs7v3rw0GjtnYsxIiEgHp0cT4vp0jNLLOJcq5kgxIBIKM4aXl6Z7/e6uxuFmzbQBW/Kf//5\ne//yIae2Oz+8sLiwVMwK5hCdaxlTVN00q+rq/PzcaE1EKlOUJitXrlxYuFBXVVkWmklFBoWogJlj\n5Ogb7xvPwuOF0fOvX3mtrU5OJqcnxcHe0d2HGwJqobeokzTJch9DVZWRgyKKzrchINLLa2urK8vd\nvGOUCsHrzKSM0WitUTnlCXkKMfhACpYvX5xf6I4pX7o4mpzPLl1enE2q09NpsesJzHCQH+4fFmdn\ng+FwNByX00Kh7/f6CwvzRhtEJKUMkbZak9KEJBARmVC8dyI+y9JuPxMKwgIxaoBBnvfSdDzff9Ls\nc8CleT3uLT49sCeTMs87wQcf4iuvvbK6uhp8sMZwCCEELZEFCREiM3MEwLbx1prxaGgS7aJvmxaF\n0iQh8YE9Gd3p2W6nf2ElZ+De5c7W9vHkoKqPy7zXfenVV4fDYVM3IAJaM7NWiW2q2gOAiAiFKCB6\nPJrvDwazqg7YEKEGBRwQCATbps2STlW1Z20wGWFGy8/3B4OsOJu+eO36sysrMTiJHENQRjOLds6J\nwG8fQyvLjNY4oyfOx8lZmRowKTGRUgSRAIBFRwiOI2ButDHMpNBc0K+9cePNm3+YZFlZeFKKIxMq\noaBj7RQZo62IeA/lNDR10xmqTjcdzHXzfhQVSGlAI0LkfZfSs9N2bm7QG+bGoAJCENLy3M2Xrz7/\nQusDKOVFArNSRgQ1AooIAkaIAQOmcf7ZzsXnOkop1GLSIGwATWAJUVBZpe3V66uEvuWajE6T1Cit\nW1iYW3R1ANLaJEq7yBxJEEhroxFUiMGJY4w6A5Mr+n+BVQQhBAAgUhoFSQdkAA/iCSDE6L2PIRjq\nzY+GTVG3jiMAorFGiwiI+20BAxAyoAALhxgTZfM0B62atkGJRBhFondKyCoIUURQhIQhtM4ay7F9\nsnGvk4yTrNO1vcyayFTXIYroGCOhZo4cBASUNkgaBYxWDGwIRQgQY2g5snCQyIkyhGnNjUkNChAi\nAJblgW/OTGM7WV9hlmSj4aCvZKhZxBqDgooYQVx0nltlyHPLLNFHRBWiD84r0gpNBAcAzECkJYJV\nlgA5BGM0KnChliJolVTNcQjKqM7/AW5nzMjOAWjiAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=RGB size=32x32 at 0x7F8E80652F98>"]},"metadata":{"tags":[]},"execution_count":104}]}]}